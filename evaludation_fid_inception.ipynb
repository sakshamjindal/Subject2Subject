{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0931a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload modules before executing cells\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1b6199-9dfe-4055-8a84-66ff4bfa8901",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from diffusers import DiffusionPipeline, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be0a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DreamBoothDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_data_root,\n",
    "        instance_prompt,\n",
    "        tokenizer,\n",
    "        class_data_root=None,\n",
    "        class_prompt=None,\n",
    "        size=512,\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.instance_data_root = Path(instance_data_root)\n",
    "        if not self.instance_data_root.exists():\n",
    "            raise ValueError(\"Instance images root doesn't exists.\")\n",
    "\n",
    "        self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
    "        self.num_instance_images = len(self.instance_images_path)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self._length = self.num_instance_images\n",
    "\n",
    "        if class_data_root is not None:\n",
    "            self.class_data_root = Path(class_data_root)\n",
    "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
    "            self.class_images_path = list(Path(class_data_root).iterdir())\n",
    "            self.num_class_images = len(self.class_images_path)\n",
    "            self._length = max(self.num_class_images, self.num_instance_images)\n",
    "            self.class_prompt = class_prompt\n",
    "        else:\n",
    "            self.class_data_root = None\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            self.instance_prompt,\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "        ).input_ids\n",
    "\n",
    "        if self.class_data_root:\n",
    "            class_image = Image.open(self.class_images_path[index % self.num_class_images])\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.image_transforms(class_image)\n",
    "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
    "                self.class_prompt,\n",
    "                padding=\"do_not_pad\",\n",
    "                truncation=True,\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "            ).input_ids\n",
    "        \n",
    "        return example[\"instance_images\"]\n",
    "\n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, prompt, num_samples):\n",
    "        self.prompt = prompt\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        example[\"prompt\"] = self.prompt\n",
    "        example[\"index\"] = index\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a483a70b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133bbce965a14aba9775ef10f0b1910f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 3, 512, 512]) torch.Size([18, 3, 512, 512])\n",
      "====> For the train data loader:\n",
      "FID score: 0.030175975406464967\n",
      "Inception score: 1.7770968250203503\n"
     ]
    }
   ],
   "source": [
    "LOW_RESOURCE = False \n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_id = \"./logs/dog/prior_high_lr_again\"\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(model_id).to(device)\n",
    "tokenizer = ldm_stable.tokenizer\n",
    "\n",
    "###########\n",
    "instance_data_dir = \"./data/dog/original\"\n",
    "instance_prompt = \"a photo of <sks-dog>\"\n",
    "\n",
    "train_dataset = DreamBoothDataset(\n",
    "    instance_data_root=instance_data_dir,\n",
    "    instance_prompt=instance_prompt,\n",
    "    class_data_root=None,\n",
    "    class_prompt=None,\n",
    "    tokenizer=tokenizer,\n",
    "    size=512,\n",
    "    center_crop=True,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=len(train_dataset),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "#####\n",
    "import PIL.Image as Image\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import FID, InceptionScore\n",
    "\n",
    "fid_metric = FID(device=device)\n",
    "is_metric = InceptionScore(device=device, output_transform=lambda x: x[0])\n",
    "\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transforms.ToPILImage()(img.cpu())\n",
    "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
    "        arr.append(transforms.ToTensor()(resized_img))\n",
    "    return torch.stack(arr)\n",
    "\n",
    "\n",
    "def evaluation_step(engine, batch):\n",
    "    with torch.no_grad():\n",
    "        negative_prompt = \"low quality, blurry, unfinished\"\n",
    "        fake_batch = ldm_stable(instance_prompt , num_images_per_prompt=len(train_dataset), output_type=\"pil\", negative_prompt =negative_prompt).images\n",
    "        fake_batch = torch.stack([transforms.ToTensor()(img) for img in fake_batch])\n",
    "        print(fake_batch.shape, batch.shape)\n",
    "        fake = interpolate(fake_batch)\n",
    "        real = interpolate(batch)\n",
    "        return fake, real\n",
    "\n",
    "evaluator = Engine(evaluation_step)\n",
    "fid_metric.attach(evaluator, \"fid\")\n",
    "is_metric.attach(evaluator, \"is\")\n",
    "\n",
    "# run the evaluator on the val data loader\n",
    "evaluator.run(train_dataloader, max_epochs=1) \n",
    "metrics = evaluator.state.metrics\n",
    "fid_score = metrics['fid']\n",
    "is_score = metrics['is']\n",
    "print(\"====> For the train data loader:\")\n",
    "print(\"FID score: {}\".format(fid_score))\n",
    "print(\"Inception score: {}\".format(is_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e167f692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df676cf96a94a8ab695d913938d24c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 3, 512, 512]) torch.Size([18, 3, 512, 512])\n",
      "====> For the train data loader:\n",
      "FID score: 0.1030309235339662\n",
      "Inception score: 1.5083089183497504\n"
     ]
    }
   ],
   "source": [
    "LOW_RESOURCE = False \n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(model_id).to(device)\n",
    "ldm_stable.load_textual_inversion(\"./logs/dog/text_inv\")\n",
    "tokenizer = ldm_stable.tokenizer\n",
    "\n",
    "###########\n",
    "instance_data_dir = \"./data/dog/original\"\n",
    "instance_prompt = \"a photo of <sks-dog>\"\n",
    "\n",
    "train_dataset = DreamBoothDataset(\n",
    "    instance_data_root=instance_data_dir,\n",
    "    instance_prompt=instance_prompt,\n",
    "    class_data_root=None,\n",
    "    class_prompt=None,\n",
    "    tokenizer=tokenizer,\n",
    "    size=512,\n",
    "    center_crop=True,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=len(train_dataset),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "#####\n",
    "import PIL.Image as Image\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import FID, InceptionScore\n",
    "\n",
    "fid_metric = FID(device=device)\n",
    "is_metric = InceptionScore(device=device, output_transform=lambda x: x[0])\n",
    "\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transforms.ToPILImage()(img.cpu())\n",
    "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
    "        arr.append(transforms.ToTensor()(resized_img))\n",
    "    return torch.stack(arr)\n",
    "\n",
    "\n",
    "def evaluation_step(engine, batch):\n",
    "    with torch.no_grad():\n",
    "        negative_prompt = \"low quality, blurry, unfinished\"\n",
    "        fake_batch = ldm_stable(instance_prompt , num_images_per_prompt=len(train_dataset), output_type=\"pil\", negative_prompt =negative_prompt).images\n",
    "        fake_batch = torch.stack([transforms.ToTensor()(img) for img in fake_batch])\n",
    "        print(fake_batch.shape, batch.shape)\n",
    "        fake = interpolate(fake_batch)\n",
    "        real = interpolate(batch)\n",
    "        return fake, real\n",
    "\n",
    "evaluator = Engine(evaluation_step)\n",
    "fid_metric.attach(evaluator, \"fid\")\n",
    "is_metric.attach(evaluator, \"is\")\n",
    "\n",
    "# run the evaluator on the val data loader\n",
    "evaluator.run(train_dataloader, max_epochs=1) \n",
    "metrics = evaluator.state.metrics\n",
    "fid_score = metrics['fid']\n",
    "is_score = metrics['is']\n",
    "print(\"====> For the train data loader:\")\n",
    "print(\"FID score: {}\".format(fid_score))\n",
    "print(\"Inception score: {}\".format(is_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1badc4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342a1e614c164e9b9ddb678d0e04640b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 3, 512, 512]) torch.Size([18, 3, 512, 512])\n",
      "====> For the train data loader:\n",
      "FID score: 0.0519351787889373\n",
      "Inception score: 1.5925326350414324\n"
     ]
    }
   ],
   "source": [
    "LOW_RESOURCE = False \n",
    "NUM_DIFFUSION_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_id = \"./logs/dog/lora\"\n",
    "ldm_stable = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to(device)\n",
    "ldm_stable.load_lora_weights(model_id)\n",
    "tokenizer = ldm_stable.tokenizer\n",
    "\n",
    "###########\n",
    "instance_data_dir = \"./data/dog/original\"\n",
    "instance_prompt = \"a photo of <sks-dog>\"\n",
    "\n",
    "train_dataset = DreamBoothDataset(\n",
    "    instance_data_root=instance_data_dir,\n",
    "    instance_prompt=instance_prompt,\n",
    "    class_data_root=None,\n",
    "    class_prompt=None,\n",
    "    tokenizer=tokenizer,\n",
    "    size=512,\n",
    "    center_crop=True,\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=len(train_dataset),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "#####\n",
    "import PIL.Image as Image\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import FID, InceptionScore\n",
    "\n",
    "fid_metric = FID(device=device)\n",
    "is_metric = InceptionScore(device=device, output_transform=lambda x: x[0])\n",
    "\n",
    "def interpolate(batch):\n",
    "    arr = []\n",
    "    for img in batch:\n",
    "        pil_img = transforms.ToPILImage()(img.cpu())\n",
    "        resized_img = pil_img.resize((299,299), Image.BILINEAR)\n",
    "        arr.append(transforms.ToTensor()(resized_img))\n",
    "    return torch.stack(arr)\n",
    "\n",
    "\n",
    "def evaluation_step(engine, batch):\n",
    "    with torch.no_grad():\n",
    "        negative_prompt = \"low quality, blurry, unfinished\"\n",
    "        fake_batch = ldm_stable(instance_prompt , num_images_per_prompt=len(train_dataset), output_type=\"pil\", negative_prompt =negative_prompt).images\n",
    "        fake_batch = torch.stack([transforms.ToTensor()(img) for img in fake_batch])\n",
    "        print(fake_batch.shape, batch.shape)\n",
    "        fake = interpolate(fake_batch)\n",
    "        real = interpolate(batch)\n",
    "        return fake, real\n",
    "\n",
    "evaluator = Engine(evaluation_step)\n",
    "fid_metric.attach(evaluator, \"fid\")\n",
    "is_metric.attach(evaluator, \"is\")\n",
    "\n",
    "# run the evaluator on the val data loader\n",
    "evaluator.run(train_dataloader, max_epochs=1) \n",
    "metrics = evaluator.state.metrics\n",
    "fid_score = metrics['fid']\n",
    "is_score = metrics['is']\n",
    "print(\"====> For the train data loader:\")\n",
    "print(\"FID score: {}\".format(fid_score))\n",
    "print(\"Inception score: {}\".format(is_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdca327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m97",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m97"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
